# Object Detection for Visually Impaired Using Machine Learning

## üìå Overview
This project is a real-time, low-cost assistive system designed to help visually impaired individuals by providing audio-based awareness of their surroundings. It detects objects, recognizes familiar faces, estimates distance, and delivers voice feedback, enabling safer and more independent navigation.

The system is optimized to run **offline on CPU-only systems**, making it suitable for low-resource environments without internet or GPU dependency.

## ‚ú® Features
- Real-time object detection using **YOLOv3-tiny** trained on the **COCO dataset**
- Face detection using **Haar Cascade Classifier**
- Face recognition and user registration using **LBPH**
- Voice-based feedback and alerts using **pyttsx3**
- Voice-based user registration (hands-free interaction)
- Distance estimation using the **pinhole camera model**
- Offline, lightweight, and CPU-efficient execution

## üõ†Ô∏è Technologies Used
- Python  
- OpenCV  
- YOLOv3-tiny (COCO Models)  
- Haar Cascade  
- LBPH Face Recognizer  
- TensorFlow  
- pyttsx3 (Text-to-Speech)  
- Google Speech Recognition API / CMU Sphinx (offline fallback)

## ‚ñ∂Ô∏è How It Works
1. Camera captures real-time video frames  
2. YOLOv3-tiny detects objects in the scene  
3. Haar Cascade detects faces and LBPH recognizes registered users  
4. Distance to detected objects is estimated using bounding box dimensions  
5. Detected objects, faces, and distances are converted into speech  
6. Audio feedback is delivered to the user in real time  

## ‚ñ∂Ô∏è How to Run
1. Clone the repository:
   ```bash
   git clone https://github.com/sudheer72/Object-Detection-for-Visually-Impaired-Using-ML.git
````

2. Install required dependencies:

   ```bash
   pip install -r requirements.txt
   ```
3. Run the main Python file:

   ```bash
   python main.py
   ```

## üìä Performance

* Object Detection Accuracy (mAP): ~55%
* Face Recognition Accuracy: ~85%
* Frame Rate: 18‚Äì22 FPS (CPU-only execution)
* Works reliably in indoor environments under moderate lighting

## üéØ Use Case

* Assists visually impaired users in identifying nearby objects and people
* Provides real-time spatial awareness using audio cues
* Enables hands-free interaction and offline usability

## üöÄ Future Enhancements

* Support for multiple languages in voice feedback
* Improved distance estimation using depth sensors
* Deployment on wearable devices (smart glasses)
* Enhanced face recognition under low-light conditions

## üîΩ Model Files

Due to GitHub size limits, pretrained model files are not included.

Download links:

* YOLOv3: [https://pjreddie.com/media/files/yolov3.weights](https://pjreddie.com/media/files/yolov3.weights)
* YOLOv3-tiny: [https://pjreddie.com/media/files/yolov3-tiny.weights](https://pjreddie.com/media/files/yolov3-tiny.weights)
* Dlib shape predictor: [https://github.com/davisking/dlib-models](https://github.com/davisking/dlib-models)

## üë®‚Äçüíª Author

**Sudheer Ravi**
B.Tech ‚Äì Computer Science and Engineering
VIT-AP University
